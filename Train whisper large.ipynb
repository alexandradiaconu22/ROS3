{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from datasets import Audio\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "api_token = \"\"\n",
    "login(token = api_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\")\n",
    "\n",
    "def load_audio(file_path):\n",
    "    signal, sr = librosa.load(file_path, sr=16000)\n",
    "    signal_array = np.array(signal)\n",
    "    return {\"path\": file_path, \"array\": signal_array, \"sampling_rate\": sr}\n",
    "\n",
    "def load_dataset(dataframe):\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    dataset = dataset.map(\n",
    "        lambda example: {\n",
    "            \"audio\": load_audio(example[\"audio\"]),\n",
    "            \"sentence\": example[\"sentence\"]\n",
    "        },\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79624af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_csv(\"\")\n",
    "\n",
    "def load_audio(file_path):\n",
    "    signal, sr = librosa.load(file_path, sr=16000)\n",
    "    signal_array = np.array(signal)\n",
    "    return {\"path\": file_path, \"array\": signal_array, \"sampling_rate\": sr}\n",
    "\n",
    "def load_dataset(dataframe):\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    dataset = dataset.map(\n",
    "        lambda example: {\n",
    "            \"audio\": load_audio(example[\"audio\"]),\n",
    "            \"sentence\": example[\"sentence\"]\n",
    "        },\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d2cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dict = datasets.DatasetDict({\n",
    "    \"train\": load_dataset(df.iloc[:int(len(df)*1)]),\n",
    "    \"test\": load_dataset(df_valid.iloc[:int(len(df_valid)*1)])\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68464d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large-v3\", language=\"romanian\", task=\"transcribe\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\", language=\"romanian\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db53d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_dict.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc4a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = 30.0\n",
    "mil = int(mds * 16000)\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "    # compute input length\n",
    "    batch[\"input_length\"] = len(batch[\"audio\"])\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], max_length=mil).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"], truncation=True,max_length=448).input_ids\n",
    "\n",
    "    # compute labels length\n",
    "    batch[\"labels_length\"] = len(tokenizer(batch[\"sentence\"], add_special_tokens=False).input_ids)\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5835392",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_dict.map(prepare_dataset,remove_columns=data_dict.column_names[\"train\"],num_proc=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82561076",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbb1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c4f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec69177",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language = \"romanian\", task = \"transcribe\")\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c40ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"\",  \n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,  \n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",  \n",
    "    warmup_steps=3000,\n",
    "    max_steps=20000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3, \n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae253f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=data_dict[\"train\"],\n",
    "    eval_dataset=data_dict[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9182df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
